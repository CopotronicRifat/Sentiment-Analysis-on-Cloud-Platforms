{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WS90FBGnVrYj",
        "outputId": "6b3a263e-73cb-4310-d772-44471eef1fb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "    8192/17464789 [..............................] - ETA: 0s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 328s 244ms/step - loss: 0.7221 - accuracy: 0.7788 - val_loss: 0.4199 - val_accuracy: 0.8525\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 276s 221ms/step - loss: 0.3256 - accuracy: 0.8780 - val_loss: 0.3131 - val_accuracy: 0.8700\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 274s 219ms/step - loss: 0.2593 - accuracy: 0.9026 - val_loss: 0.3739 - val_accuracy: 0.8713\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 254s 203ms/step - loss: 0.2186 - accuracy: 0.9202 - val_loss: 0.4910 - val_accuracy: 0.8686\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 270s 216ms/step - loss: 0.1982 - accuracy: 0.9317 - val_loss: 0.4339 - val_accuracy: 0.8690\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 250s 200ms/step - loss: 1.6968 - accuracy: 0.9307 - val_loss: 0.9814 - val_accuracy: 0.8618\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 267s 214ms/step - loss: 0.2226 - accuracy: 0.9440 - val_loss: 0.7125 - val_accuracy: 0.8704\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 267s 214ms/step - loss: 0.0702 - accuracy: 0.9772 - val_loss: 1.0092 - val_accuracy: 0.8614\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 265s 212ms/step - loss: 0.0822 - accuracy: 0.9775 - val_loss: 1.2125 - val_accuracy: 0.8626\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 265s 212ms/step - loss: 0.1183 - accuracy: 0.9732 - val_loss: 1.3988 - val_accuracy: 0.8596\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7871ae4ee0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Download the IMDb movie review dataset\n",
        "from tensorflow.keras.datasets import imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data()\n",
        "\n",
        "# Combine train and test data for preprocessing\n",
        "data = np.concatenate((train_data, test_data), axis=0)\n",
        "labels = np.concatenate((train_labels, test_labels), axis=0)\n",
        "\n",
        "# Set vocabulary size and maximum sequence length\n",
        "vocab_size = 10000\n",
        "max_len = 256\n",
        "\n",
        "# Preprocess the text data\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts([' '.join(map(str, sentence)) for sentence in data])\n",
        "sequences = tokenizer.texts_to_sequences([' '.join(map(str, sentence)) for sentence in data])\n",
        "word_index = tokenizer.word_index\n",
        "data = pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Define the Transformer model\n",
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, max_len, num_heads, ff_dim, num_blocks, dropout_rate):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_len, output_dim=embedding_dim)\n",
        "        self.encoder_blocks = [TransformerEncoderBlock(embedding_dim, num_heads, ff_dim, dropout_rate) for _ in range(num_blocks)]\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        positions = tf.range(start=0, limit=tf.shape(x)[-2], delta=1)\n",
        "        positions = self.pos_embedding(positions)\n",
        "        x += positions\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            x = encoder_block(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_dim, num_heads, ff_dim, dropout_rate):\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
        "            tf.keras.layers.Dense(embedding_dim)\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.layernorm1(inputs)\n",
        "        attention_output = self.att(x, x)\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        x2 = tf.keras.layers.Add()([inputs, attention_output])\n",
        "        x = self.layernorm2(x2)\n",
        "        ffn_output = self.ffn(x)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        x3 = tf.keras.layers.Add()([x2, ffn_output])\n",
        "        return x3\n",
        "\n",
        "# Define hyperparameters\n",
        "embedding_dim = 128\n",
        "num_heads = 8\n",
        "ff_dim = 256\n",
        "num_blocks = 6\n",
        "dropout_rate = 0.1\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Create an instance of the Transformer model\n",
        "model = TransformerModel(vocab_size, embedding_dim, max_len, num_heads, ff_dim, num_blocks, dropout_rate)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Convert the labels to numpy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n",
        "\n"
      ]
    }
  ]
}