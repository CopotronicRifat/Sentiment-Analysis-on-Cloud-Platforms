{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN, Dense\n",
        "from keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "1yIitrB2wyqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sO97MYVSwzj2",
        "outputId": "006b57c4-83d0-4f08-d31e-d465593d2efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHcbizohu9Kk",
        "outputId": "b3689065-6c1a-4a86-c95b-5523acdc6b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "25/25 [==============================] - 38s 1s/step - loss: 0.7028 - accuracy: 0.5113 - val_loss: 0.7120 - val_accuracy: 0.5175\n",
            "Epoch 2/10\n",
            "25/25 [==============================] - 33s 1s/step - loss: 0.5754 - accuracy: 0.7231 - val_loss: 0.6981 - val_accuracy: 0.5225\n",
            "Epoch 3/10\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.3735 - accuracy: 0.9663 - val_loss: 0.7530 - val_accuracy: 0.4500\n",
            "Epoch 4/10\n",
            "25/25 [==============================] - 33s 1s/step - loss: 0.2380 - accuracy: 0.9506 - val_loss: 0.7035 - val_accuracy: 0.5350\n",
            "Epoch 5/10\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1970 - accuracy: 0.9887 - val_loss: 1.0108 - val_accuracy: 0.5075\n",
            "Epoch 6/10\n",
            "25/25 [==============================] - 39s 2s/step - loss: 0.0477 - accuracy: 0.9981 - val_loss: 0.8904 - val_accuracy: 0.5000\n",
            "Epoch 7/10\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.0214 - accuracy: 0.9981 - val_loss: 0.9382 - val_accuracy: 0.4950\n",
            "Epoch 8/10\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.0115 - accuracy: 0.9994 - val_loss: 1.4648 - val_accuracy: 0.4900\n",
            "Epoch 9/10\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 1.2098 - val_accuracy: 0.5150\n",
            "Epoch 10/10\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.3557 - val_accuracy: 0.5150\n",
            "1/1 [==============================] - 0s 234ms/step\n",
            "The predicted sentiment of the test string is: positive\n"
          ]
        }
      ],
      "source": [
        "nltk.download('movie_reviews')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def load_imdb_dataset():\n",
        "    positive_reviews = nltk.corpus.movie_reviews.fileids('pos')\n",
        "    positive_reviews = [nltk.corpus.movie_reviews.raw(fileid) for fileid in positive_reviews]\n",
        "    negative_reviews = nltk.corpus.movie_reviews.fileids('neg')\n",
        "    negative_reviews = [nltk.corpus.movie_reviews.raw(fileid) for fileid in negative_reviews]\n",
        "    dataset = [(review, 'positive') for review in positive_reviews] + [(review, 'negative') for review in negative_reviews]\n",
        "    return dataset\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "dataset = load_imdb_dataset()\n",
        "random.shuffle(dataset)\n",
        "\n",
        "reviews, labels = zip(*dataset)\n",
        "reviews = [\" \".join(preprocess_text(review)) for review in reviews]\n",
        "\n",
        "# Tokenize and pad the sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(reviews)\n",
        "sequences = tokenizer.texts_to_sequences(reviews)\n",
        "word_index = tokenizer.word_index\n",
        "data = pad_sequences(sequences)\n",
        "\n",
        "# Convert labels to numerical values and one-hot encode them\n",
        "label_dict = {'positive': 1, 'negative': 0}\n",
        "labels = [label_dict[label] for label in labels]\n",
        "labels = to_categorical(labels)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n",
        "\n",
        "# Define RNN model\n",
        "embedding_dim = 100\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=data.shape[1]))\n",
        "model.add(SimpleRNN(embedding_dim, return_sequences=False))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the RNN model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
        "\n",
        "# Test string\n",
        "test_string = \"The movie was one good\"\n",
        "test_string = \" \".join(preprocess_text(test_string))\n",
        "test_sequence = tokenizer.texts_to_sequences([test_string])\n",
        "test_data = pad_sequences(test_sequence, maxlen=data.shape[1])\n",
        "\n",
        "# Predict the sentiment\n",
        "prediction = model.predict(test_data)\n",
        "predicted_label = \"positive\" if np.argmax(prediction) == 1 else \"negative\"\n",
        "print(f\"The predicted sentiment of the test string is: {predicted_label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_1uj-135wQG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}