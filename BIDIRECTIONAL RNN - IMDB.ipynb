{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThAgQoujKRS-",
        "outputId": "2b9ef1ee-1a9c-4c9d-c9e2-5434643edbaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 499s 1s/step - loss: 0.4075 - accuracy: 0.8058 - val_loss: 0.3231 - val_accuracy: 0.8616\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 486s 1s/step - loss: 0.2707 - accuracy: 0.8898 - val_loss: 0.3398 - val_accuracy: 0.8499\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 488s 1s/step - loss: 0.2264 - accuracy: 0.9124 - val_loss: 0.3421 - val_accuracy: 0.8553\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 485s 1s/step - loss: 0.1931 - accuracy: 0.9272 - val_loss: 0.3768 - val_accuracy: 0.8483\n",
            "782/782 [==============================] - 120s 154ms/step - loss: 0.3768 - accuracy: 0.8483\n",
            "Loss: 0.37683355808258057\n",
            "Accuracy: 0.8483200073242188\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the IMDb movie review dataset\n",
        "vocabulary_size = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocabulary_size)\n",
        "\n",
        "# Convert word indices back to sentences\n",
        "word_index = imdb.get_word_index()\n",
        "index_to_word = {i: word for word, i in word_index.items()}\n",
        "X_train_sentences = [' '.join([index_to_word.get(word, '') for word in sentence]) for sentence in X_train]\n",
        "X_test_sentences = [' '.join([index_to_word.get(word, '') for word in sentence]) for sentence in X_test]\n",
        "\n",
        "# Preprocess the data\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_data(data):\n",
        "    processed_data = []\n",
        "    for sentence in data:\n",
        "        sentence = word_tokenize(sentence)\n",
        "        sentence = ' '.join([lemmatizer.lemmatize(word.lower()) for word in sentence if word.isalpha()])\n",
        "        sentence = ' '.join([word for word in sentence.split() if word not in stop_words])\n",
        "        processed_data.append(sentence)\n",
        "    return processed_data\n",
        "\n",
        "X_train = preprocess_data(X_train_sentences)\n",
        "X_test = preprocess_data(X_test_sentences)\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer(num_words=vocabulary_size)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences to the same length\n",
        "max_sequence_length = 250\n",
        "X_train = pad_sequences(X_train, maxlen=max_sequence_length)\n",
        "X_test = pad_sequences(X_test, maxlen=max_sequence_length)\n",
        "\n",
        "# Build the model\n",
        "embedding_size = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_sequence_length))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping criteria\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# Train the model\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    }
  ]
}