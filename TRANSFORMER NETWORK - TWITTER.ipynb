{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkQxX6KKkf34",
        "outputId": "04378a59-9a05-4b0f-d9e6-e9af55157945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "250/250 [==============================] - 86s 233ms/step - loss: 1.3428 - accuracy: 0.6295 - val_loss: 0.4429 - val_accuracy: 0.7945\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 52s 207ms/step - loss: 0.5955 - accuracy: 0.7933 - val_loss: 0.6149 - val_accuracy: 0.7535\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 50s 201ms/step - loss: 0.4329 - accuracy: 0.8624 - val_loss: 0.6367 - val_accuracy: 0.7735\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 50s 201ms/step - loss: 0.3125 - accuracy: 0.8994 - val_loss: 1.3399 - val_accuracy: 0.7145\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 49s 195ms/step - loss: 0.2170 - accuracy: 0.9295 - val_loss: 0.8193 - val_accuracy: 0.8010\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 49s 197ms/step - loss: 0.2133 - accuracy: 0.9361 - val_loss: 2.3801 - val_accuracy: 0.7215\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 49s 194ms/step - loss: 0.1774 - accuracy: 0.9466 - val_loss: 1.0758 - val_accuracy: 0.7840\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 49s 194ms/step - loss: 0.1470 - accuracy: 0.9529 - val_loss: 1.1279 - val_accuracy: 0.7700\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 49s 197ms/step - loss: 0.1285 - accuracy: 0.9605 - val_loss: 1.3987 - val_accuracy: 0.7815\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 49s 196ms/step - loss: 0.1403 - accuracy: 0.9592 - val_loss: 1.8702 - val_accuracy: 0.7645\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2870578f10>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "from nltk.corpus import twitter_samples\n",
        "import numpy as np\n",
        "\n",
        "# Load the Twitter Samples dataset from NLTK\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# Combine positive and negative tweets\n",
        "tweets = positive_tweets + negative_tweets\n",
        "labels = np.concatenate((np.ones(len(positive_tweets)), np.zeros(len(negative_tweets))), axis=0)\n",
        "\n",
        "# Set vocabulary size and maximum sequence length\n",
        "vocab_size = 10000\n",
        "max_len = 256\n",
        "\n",
        "# Preprocess the text data\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(tweets)\n",
        "sequences = tokenizer.texts_to_sequences(tweets)\n",
        "word_index = tokenizer.word_index\n",
        "data = pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Transformer model\n",
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, max_len, num_heads, ff_dim, num_blocks, dropout_rate):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_len, output_dim=embedding_dim)\n",
        "        self.encoder_blocks = [TransformerEncoderBlock(embedding_dim, num_heads, ff_dim, dropout_rate) for _ in range(num_blocks)]\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        positions = tf.range(start=0, limit=tf.shape(x)[-2], delta=1)\n",
        "        positions = self.pos_embedding(positions)\n",
        "        x += positions\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            x = encoder_block(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_dim, num_heads, ff_dim, dropout_rate):\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
        "            tf.keras.layers.Dense(embedding_dim)\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.layernorm1(inputs)\n",
        "        attention_output = self.att(x, x)\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        x2 = tf.keras.layers.Add()([inputs, attention_output])\n",
        "        x = self.layernorm2(x2)\n",
        "        ffn_output = self.ffn(x)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        x3 = tf.keras.layers.Add()([x2, ffn_output])\n",
        "        return x3\n",
        "\n",
        "# Define hyperparameters\n",
        "embedding_dim = 128\n",
        "num_heads = 8\n",
        "ff_dim = 256\n",
        "num_blocks = 6\n",
        "dropout_rate = 0.1\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Create an instance of the Transformer model\n",
        "model = TransformerModel(vocab_size, embedding_dim, max_len, num_heads, ff_dim, num_blocks, dropout_rate)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Convert the labels to numpy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n"
      ]
    }
  ]
}