{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dh1JjBtnsAj",
        "outputId": "9293c1da-b023-4854-de35-2098fd212a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "125/125 [==============================] - 35s 173ms/step - loss: 0.5809 - accuracy: 0.6794 - val_loss: 0.7680 - val_accuracy: 0.5970\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 9s 75ms/step - loss: 0.3908 - accuracy: 0.8224 - val_loss: 0.6080 - val_accuracy: 0.7185\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 8s 60ms/step - loss: 0.3041 - accuracy: 0.8679 - val_loss: 0.8272 - val_accuracy: 0.6470\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 0.3040 - accuracy: 0.8740 - val_loss: 0.8729 - val_accuracy: 0.6565\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 0.2397 - accuracy: 0.8985 - val_loss: 0.8557 - val_accuracy: 0.6925\n",
            "313/313 [==============================] - 7s 21ms/step - loss: 0.3315 - accuracy: 0.8807\n",
            "Loss: 0.3314696252346039\n",
            "Accuracy: 0.8806999921798706\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('twitter_samples')\n",
        "\n",
        "# Load the Twitter Samples dataset\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# Preprocess the data\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_data(data):\n",
        "    processed_data = []\n",
        "    for tweet in data:\n",
        "        tweet = word_tokenize(tweet)\n",
        "        tweet = ' '.join([lemmatizer.lemmatize(word.lower()) for word in tweet if word.isalpha()])\n",
        "        tweet = ' '.join([word for word in tweet.split() if word not in stop_words])\n",
        "        processed_data.append(tweet)\n",
        "    return processed_data\n",
        "\n",
        "positive_tweets = preprocess_data(positive_tweets)\n",
        "negative_tweets = preprocess_data(negative_tweets)\n",
        "\n",
        "# Combine positive and negative tweets and create labels\n",
        "tweets = positive_tweets + negative_tweets\n",
        "labels = [1] * len(positive_tweets) + [0] * len(negative_tweets)\n",
        "\n",
        "# Tokenize the tweets\n",
        "vocabulary_size = 5000\n",
        "tokenizer = Tokenizer(num_words=vocabulary_size)\n",
        "tokenizer.fit_on_texts(tweets)\n",
        "sequences = tokenizer.texts_to_sequences(tweets)\n",
        "\n",
        "# Pad sequences to the same length\n",
        "max_sequence_length = 250\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "padded_sequences = np.array(padded_sequences)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Build the model\n",
        "embedding_size = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_sequence_length))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping criteria\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# Train the model\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "model.fit(padded_sequences, labels, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_sequences, labels)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n"
      ]
    }
  ]
}